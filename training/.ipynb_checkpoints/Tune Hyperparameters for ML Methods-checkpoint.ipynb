{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### VAE DR + CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "# tf.config.experimental_list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###Variational Autoencoder to get the latent layer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import keras\n",
    "\n",
    "import pydot\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT1\n",
      "CHECKPOINT2\n",
      "CHECKPOINT3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.6/multiprocessing/popen_fork.py:73: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  code = process_obj._bootstrap()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 747668)\n",
      "(436,)\n",
      "StratifiedSampling check\n",
      "Oversampling check\n",
      "Scaling check\n",
      "Returning check\n"
     ]
    }
   ],
   "source": [
    "def set_Data(data):\n",
    "    ppmi = pd.read_csv('../../datasets/preprocessed/trans_processed_PPMI_data.csv')\n",
    "    ppmi.rename(columns={'Unnamed: 0':'Sentrix_position'}, inplace=True)\n",
    "    ppmi.set_index('Sentrix_position', inplace=True)\n",
    "    ppmi = ppmi.transpose()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    label = encoder.fit_transform(ppmi['Category'])\n",
    "\n",
    "    tr = ppmi.drop(['Category'], axis=1)\n",
    "    X = tr.values\n",
    "    y = label\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    print(\"StratifiedSampling check\")\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    split.get_n_splits(X, y)\n",
    "\n",
    "    for train_index, test_index in split.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, data['y_test'] = y[train_index], y[test_index]\n",
    "\n",
    "    print(\"Oversampling check\")\n",
    "    oversampler = SMOTE(random_state=42)\n",
    "    X_train_sampled, data['y_train_sampled'] = oversampler.fit_resample(X_train, y_train)\n",
    "    print(\"Scaling check\")\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sampled)\n",
    "    data['X_train_scaled_1'] = X_train_scaled[:247].reshape((1, -1))\n",
    "    data['X_train_scaled_2'] = X_train_scaled[247:].reshape((1, -1))\n",
    "    data['X_test_scaled'] = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Returning check\")\n",
    "\n",
    "manager = Manager()\n",
    "data = manager.dict()\n",
    "\n",
    "print(\"CHECKPOINT1\")\n",
    "#     p = Process(target=set_Data, args=(X_train_scaled, X_test_scaled, y_train_sampled, y_test,))\n",
    "p = Process(target=set_Data, args=(data,))\n",
    "print(\"CHECKPOINT2\")\n",
    "p.start()\n",
    "print(\"CHECKPOINT3\")\n",
    "p.join()\n",
    "\n",
    "y_train = data['y_train_sampled']\n",
    "y_test = data['y_test']\n",
    "X_train = np.append(data['X_train_scaled_1'], data['X_train_scaled_2']).reshape(494, 747668)\n",
    "X_test = data['X_test_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom variational layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    This function is borrowed from:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise variables and Set hyperparameters\n",
    "original_dim = X_train.shape[1] #747668\n",
    "latent_dim = 100\n",
    "\n",
    "batch_size = 50 # controls the number of training samples to \n",
    "                # work through before the model's internal parameters are updated\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnaseq_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9909d7591878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Model to compress input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnaseq_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mean_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnaseq_input' is not defined"
     ]
    }
   ],
   "source": [
    "# VAE model = encoder + decoder\n",
    "\n",
    "############ENCODER##################\n",
    "inputs = Input(shape=(original_dim, ), name='encoder_input')\n",
    "#mean and log_var are the vectors of size `latent_dim`\n",
    "z_mean_linear = Dense(latent_dim, kernel_initializer='glorot_uniform', name='z_mean')(inputs)\n",
    "z_mean_batchnorm = BatchNormalization()(z_mean_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_batchnorm)\n",
    "\n",
    "z_log_var_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(inputs)\n",
    "z_log_var_batchnorm = BatchNormalization()(z_log_var_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_batchnorm)\n",
    "\n",
    "# return the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a `latent_dim` output\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "\n",
    "#### actually not very sure how and why use sampling??#####\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "# Model to compress input\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "\n",
    "############DECODER##################\n",
    "decoder_to_reconstruct = Dense(original_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "reconstructed_output = decoder_to_reconstruct(z)\n",
    "\n",
    "########### instantiate VAE model##########\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([inputs, reconstructed_output])\n",
    "vae = Model(inputs, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "    \n",
    "#Train the model\n",
    "vae.fit(X_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, X_test))\n",
    "\n",
    "score = vae.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import talos\n",
    "from talos.utils.gpu_utils import multi_gpu\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hyperparameters for MLP\n",
    "p = {'lr': (0.8, 1.0, 3),\n",
    "     'first_neuron':[256, 512],#,32, 64, 128\n",
    "     'hidden_layer_neuron':[32], #64, 128, 256\n",
    "     'batch_size': [10], #, 20, 30\n",
    "     'epochs': [10], #, 20, 30\n",
    "     'dropout': [0],#this means try every .1 value between 0 and .5\n",
    "     'kernel_initializer': ['uniform'], #,'normal'\n",
    "     'optimizer': ['Adam'], #'Nadam'\n",
    "     'losses': ['binary_crossentropy'],\n",
    "     'activation':['relu'], #, 'elu', 'tanh'\n",
    "     'last_activation': ['sigmoid'] #, 'softmax'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\";  \n",
    "def mlp_model(x_train, y_train, x_val, y_val, params):\n",
    "    # Build the model.\n",
    "    # Anyhow give parameters first\n",
    "    mlp = Sequential([\n",
    "      Dense(params['first_neuron'], activation=params['activation'], input_shape=(747668,), kernel_initializer=params['kernel_initializer']),\n",
    "      Dropout(params['dropout'], seed=42),\n",
    "      Dense(params['hidden_layer_neuron'], activation=params['activation']),\n",
    "      Dropout(params['dropout'], seed=42),\n",
    "    #   Dense(8, activation='relu'),\n",
    "    #   Dropout(Dropout(0.50, seed=42)),\n",
    "      Dense(2, activation=params['last_activation']),\n",
    "    ])\n",
    "\n",
    "    # split a single job to multiple GPUs\n",
    "#     mlp = multi_gpu(mlp)\n",
    "\n",
    "    # Compile the model\n",
    "    mlp.compile(\n",
    "      optimizer=params['optimizer'],\n",
    "      loss=params['losses'],\n",
    "      metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    # Train the data\n",
    "    history = mlp.fit(\n",
    "      x_train, # training data\n",
    "      to_categorical(y_train), # training targets\n",
    "      epochs=params['epochs'],\n",
    "      batch_size=params['batch_size'],\n",
    "    )\n",
    "\n",
    "    return history, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 99.2364 - accuracy: 0.5720\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 50.4819 - accuracy: 0.7737\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 26.3427 - accuracy: 0.8148\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 6.0931 - accuracy: 0.9300\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 6.6742 - accuracy: 0.9465\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 5.2511 - accuracy: 0.9486\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 9.7458 - accuracy: 0.9342\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.9741 - accuracy: 0.9527\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 6.6655 - accuracy: 0.9465\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.7224 - accuracy: 0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 1/6 [03:45<18:46, 225.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 162.5748 - accuracy: 0.5597\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 83.9551 - accuracy: 0.7922\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 40.1386 - accuracy: 0.8683\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 23.6784 - accuracy: 0.9321\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 31.4243 - accuracy: 0.9403\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 4.7200 - accuracy: 0.9588\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 11.4621 - accuracy: 0.9506\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.9521 - accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 6.0054 - accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 2.5066 - accuracy: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [07:29<15:00, 225.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 116.8592 - accuracy: 0.5597\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 55.3527 - accuracy: 0.7634\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 18.3971 - accuracy: 0.8313\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 4.8842 - accuracy: 0.8909\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 1.0156 - accuracy: 0.9136\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 0.4847 - accuracy: 0.9280\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.4403 - accuracy: 0.9342\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.6235 - accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.6140 - accuracy: 0.9383\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 0.3334 - accuracy: 0.9362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 3/6 [11:14<11:15, 225.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 169.2561 - accuracy: 0.5802\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 78.4825 - accuracy: 0.7387\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 23.3497 - accuracy: 0.8230\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 11.9267 - accuracy: 0.8889\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 5.2280 - accuracy: 0.9053\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 43s 179ms/step - loss: 0.4986 - accuracy: 0.9095\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 0.2300 - accuracy: 0.8971\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.1773 - accuracy: 0.8642\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.1363 - accuracy: 0.9136\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.1212 - accuracy: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [18:41<09:43, 291.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 171.5801 - accuracy: 0.5802\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 45.4014 - accuracy: 0.7243\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 9.8214 - accuracy: 0.7942\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 8.6678 - accuracy: 0.8004\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 2.4885 - accuracy: 0.8086\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 2.0026 - accuracy: 0.7963\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 1.3848 - accuracy: 0.8066\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 44s 183ms/step - loss: 0.3892 - accuracy: 0.8045\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 0.3906 - accuracy: 0.8025\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.3893 - accuracy: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [26:12<05:39, 339.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 43s 179ms/step - loss: 169.7645 - accuracy: 0.6111\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 36.7431 - accuracy: 0.7922\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 11.6468 - accuracy: 0.8683\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 183ms/step - loss: 2.3361 - accuracy: 0.8807\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 1.4793 - accuracy: 0.8992\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.2897 - accuracy: 0.9198\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.1192 - accuracy: 0.9239\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 45s 183ms/step - loss: 0.1588 - accuracy: 0.9239\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 0.3978 - accuracy: 0.9177\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 45s 184ms/step - loss: 1.7546 - accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6/6 [34:00<00:00, 340.03s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#Tune hyperparameters using talos\n",
    "## To start fast, limit the permutation to 1/100 of the original permutation\n",
    "scan_object = talos.Scan(x=X_train,\n",
    "                         y=y_train, \n",
    "                         params=p,\n",
    "                         model=mlp_model,\n",
    "                         experiment_name='mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.722449</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.333380</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.934156</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.389267</td>\n",
       "      <td>0.804527</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1.754559</td>\n",
       "      <td>0.911523</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs      loss  accuracy activation  batch_size  dropout  epochs  \\\n",
       "0            10  2.722449  0.958848       relu          10        0      10   \n",
       "1            10  2.506606  0.971193       relu          10        0      10   \n",
       "2            10  0.333380  0.936214       relu          10        0      10   \n",
       "3            10  0.121178  0.934156       relu          10        0      10   \n",
       "4            10  0.389267  0.804527       relu          10        0      10   \n",
       "5            10  1.754559  0.911523       relu          10        0      10   \n",
       "\n",
       "   first_neuron  hidden_layer_neuron kernel_initializer last_activation  \\\n",
       "0           256                   32            uniform         sigmoid   \n",
       "1           256                   32            uniform         sigmoid   \n",
       "2           256                   32            uniform         sigmoid   \n",
       "3           512                   32            uniform         sigmoid   \n",
       "4           512                   32            uniform         sigmoid   \n",
       "5           512                   32            uniform         sigmoid   \n",
       "\n",
       "                losses        lr optimizer  \n",
       "0  binary_crossentropy  0.800000      Adam  \n",
       "1  binary_crossentropy  0.866667      Adam  \n",
       "2  binary_crossentropy  0.933333      Adam  \n",
       "3  binary_crossentropy  0.800000      Adam  \n",
       "4  binary_crossentropy  0.866667      Adam  \n",
       "5  binary_crossentropy  0.933333      Adam  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.612864</td>\n",
       "      <td>2.292603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.627731</td>\n",
       "      <td>2.292070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.081373</td>\n",
       "      <td>2.292943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.102660</td>\n",
       "      <td>2.294472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954518</td>\n",
       "      <td>2.298395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.808678</td>\n",
       "      <td>2.296348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  1.612864  2.292603\n",
       "1  1.627731  2.292070\n",
       "2  1.081373  2.292943\n",
       "3  1.102660  2.294472\n",
       "4  0.954518  2.298395\n",
       "5  0.808678  2.296348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "experiment_name                     mlp\n",
       "random_method          uniform_mersenne\n",
       "reduction_method                   None\n",
       "reduction_interval                   50\n",
       "reduction_window                     20\n",
       "reduction_threshold                 0.2\n",
       "reduction_metric                val_acc\n",
       "complete_time            02/03/20/22:25\n",
       "x_shape                   (348, 747668)\n",
       "y_shape                          (348,)\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing the results data frame\n",
    "display(scan_object.data)\n",
    "\n",
    "# accessing epoch entropy values for each round\n",
    "display(scan_object.learning_entropy)\n",
    "\n",
    "# access the summary details\n",
    "scan_object.details\n",
    "\n",
    "####Documentation for training: as the number of neuron increases, accuracy does not necessarily increase, however the time taken for training increases significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.722449</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.333380</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.934156</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1.754559</td>\n",
       "      <td>0.911523</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.389267</td>\n",
       "      <td>0.804527</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs      loss  accuracy activation  batch_size  dropout  epochs  \\\n",
       "1            10  2.506606  0.971193       relu          10        0      10   \n",
       "0            10  2.722449  0.958848       relu          10        0      10   \n",
       "2            10  0.333380  0.936214       relu          10        0      10   \n",
       "3            10  0.121178  0.934156       relu          10        0      10   \n",
       "5            10  1.754559  0.911523       relu          10        0      10   \n",
       "4            10  0.389267  0.804527       relu          10        0      10   \n",
       "\n",
       "   first_neuron  hidden_layer_neuron kernel_initializer last_activation  \\\n",
       "1           256                   32            uniform         sigmoid   \n",
       "0           256                   32            uniform         sigmoid   \n",
       "2           256                   32            uniform         sigmoid   \n",
       "3           512                   32            uniform         sigmoid   \n",
       "5           512                   32            uniform         sigmoid   \n",
       "4           512                   32            uniform         sigmoid   \n",
       "\n",
       "                losses        lr optimizer  \n",
       "1  binary_crossentropy  0.866667      Adam  \n",
       "0  binary_crossentropy  0.800000      Adam  \n",
       "2  binary_crossentropy  0.933333      Adam  \n",
       "3  binary_crossentropy  0.800000      Adam  \n",
       "5  binary_crossentropy  0.933333      Adam  \n",
       "4  binary_crossentropy  0.866667      Adam  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_so = scan_object.data.sort_values(by=[\"accuracy\"], ascending=False)\n",
    "sorted_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2.908766e-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606e+00</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs          loss  accuracy activation  batch_size  dropout  \\\n",
       "3            10  2.908766e-14  1.000000       relu          10        0   \n",
       "1            10  2.506606e+00  0.971193       relu          10        0   \n",
       "\n",
       "   epochs  first_neuron  hidden_layer_neuron kernel_initializer  \\\n",
       "3      10            64                   32            uniform   \n",
       "1      10           256                   32            uniform   \n",
       "\n",
       "  last_activation               losses        lr optimizer  \n",
       "3         sigmoid  binary_crossentropy  0.800000      Adam  \n",
       "1         sigmoid  binary_crossentropy  0.866667      Adam  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2.908766e-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs          loss  accuracy activation  batch_size  dropout  \\\n",
       "3            10  2.908766e-14       1.0       relu          10        0   \n",
       "\n",
       "   epochs  first_neuron  hidden_layer_neuron kernel_initializer  \\\n",
       "3      10            64                   32            uniform   \n",
       "\n",
       "  last_activation               losses   lr optimizer  \n",
       "3         sigmoid  binary_crossentropy  0.8      Adam  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(sorted_so.iloc[:1])\n",
    "# final_params_info = df1\n",
    "df = pd.concat([df1, final_params_info]).sort_values(by=['accuracy'], ascending=False)\n",
    "display(df)\n",
    "\n",
    "final_params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear GPU memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100.73245377974077, 0.6477272510528564]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on Test set\n",
    "# evaluate() returns an array containing the test loss followed by any metrics we specified. \n",
    "mlp.evaluate(\n",
    "  X_test,\n",
    "  to_categorical(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import ctypes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from umap.umap_ import UMAP\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from multiprocessing import Process, Manager, RawArray, Array\n",
    "\n",
    "\n",
    "def set_Data(data):\n",
    "    ppmi = pd.read_csv('../../datasets/preprocessed/trans_processed_PPMI_data.csv')\n",
    "    ppmi.rename(columns={'Unnamed: 0':'Sentrix_position'}, inplace=True)\n",
    "    ppmi.set_index('Sentrix_position', inplace=True)\n",
    "    ppmi = ppmi.transpose()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    label = encoder.fit_transform(ppmi['Category'])\n",
    "\n",
    "    tr = ppmi.drop(['Category'], axis=1)\n",
    "    X = tr.values\n",
    "    y = label\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    print(\"StratifiedSampling check\")\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=29)\n",
    "    split.get_n_splits(X, y)\n",
    "\n",
    "    for train_index, test_index in split.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, data['y_test'] = y[train_index], y[test_index]\n",
    "\n",
    "    print(\"Oversampling check\")\n",
    "    oversampler = SMOTE(random_state=42)\n",
    "    X_train_sampled, data['y_train_sampled'] = oversampler.fit_resample(X_train, y_train)\n",
    "    print(\"Scaling check\")\n",
    "    scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sampled)\n",
    "    print(\"Shape of X_train_sacled before splitting\", X_train_scaled.shape)\n",
    "    data['X_train_scaled_1'] = X_train_scaled[:247].reshape((1, -1))\n",
    "    data['X_train_scaled_2'] = X_train_scaled[247:].reshape((1, -1))\n",
    "    data['X_test_scaled'] = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Returning check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT1\n",
      "CHECKPOINT2\n",
      "CHECKPOINT3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.6/multiprocessing/popen_fork.py:73: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  code = process_obj._bootstrap()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 747668)\n",
      "(436,)\n",
      "StratifiedSampling check\n",
      "Oversampling check\n",
      "Scaling check\n",
      "Shape of X_train_sacled before splitting (432, 747668)\n",
      "Returning check\n"
     ]
    }
   ],
   "source": [
    "manager = Manager()\n",
    "data = manager.dict()\n",
    "\n",
    "print(\"CHECKPOINT1\")\n",
    "#     p = Process(target=set_Data, args=(X_train_scaled, X_test_scaled, y_train_sampled, y_test,))\n",
    "p = Process(target=set_Data, args=(data,))\n",
    "print(\"CHECKPOINT2\")\n",
    "p.start()\n",
    "print(\"CHECKPOINT3\")\n",
    "p.join()\n",
    "\n",
    "y_train_sampled = data['y_train_sampled']\n",
    "y_test = data['y_test']\n",
    "X_train_scaled = np.append(data['X_train_scaled_1'], data['X_train_scaled_2']).reshape(432, 747668)\n",
    "X_test_scaled = data['X_test_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final train and test sets: (432, 747668) (131, 747668)\n",
      "Shape of X train: (432, 20)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.538347102863232\n",
      "With UMAP= 20 and l1, the best params are:\n",
      "{'C': 0.1} for n_compo= 20\n",
      "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[114 102]\n",
      " [ 83 133]]\n",
      "[[12 26]\n",
      " [34 59]]\n",
      "precision of testing set: 0.6941176470588235\n",
      "recall of testing set 0.6344086021505376\n",
      "accuracy of testing set 0.5419847328244275\n",
      "f1 of testing set 0.6629213483146066\n",
      "Shape of X train: (432, 25)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.546622326551904\n",
      "With UMAP= 25 and l1, the best params are:\n",
      "{'C': 100} for n_compo= 25\n",
      "LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[ 66 150]\n",
      " [ 39 177]]\n",
      "[[ 9 29]\n",
      " [18 75]]\n",
      "precision of testing set: 0.7211538461538461\n",
      "recall of testing set 0.8064516129032258\n",
      "accuracy of testing set 0.6412213740458015\n",
      "f1 of testing set 0.7614213197969544\n",
      "Shape of X train: (432, 30)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5278219395866454\n",
      "With UMAP= 30 and l1, the best params are:\n",
      "{'C': 1} for n_compo= 30\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[115 101]\n",
      " [ 83 133]]\n",
      "[[15 23]\n",
      " [34 59]]\n",
      "precision of testing set: 0.7195121951219512\n",
      "recall of testing set 0.6344086021505376\n",
      "accuracy of testing set 0.5648854961832062\n",
      "f1 of testing set 0.6742857142857142\n",
      "Shape of X train: (432, 35)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5360046457607434\n",
      "With UMAP= 35 and l1, the best params are:\n",
      "{'C': 1} for n_compo= 35\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[106 110]\n",
      " [ 76 140]]\n",
      "[[13 25]\n",
      " [32 61]]\n",
      "precision of testing set: 0.7093023255813954\n",
      "recall of testing set 0.6559139784946236\n",
      "accuracy of testing set 0.5648854961832062\n",
      "f1 of testing set 0.6815642458100559\n",
      "Shape of X train: (432, 40)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5387857617352232\n",
      "With UMAP= 40 and l1, the best params are:\n",
      "{'C': 1} for n_compo= 40\n",
      "LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[112 104]\n",
      " [ 75 141]]\n",
      "[[12 26]\n",
      " [35 58]]\n",
      "precision of testing set: 0.6904761904761905\n",
      "recall of testing set 0.6236559139784946\n",
      "accuracy of testing set 0.5343511450381679\n",
      "f1 of testing set 0.655367231638418\n",
      "Shape of X train: (432, 45)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5307615480649188\n",
      "With UMAP= 45 and l1, the best params are:\n",
      "{'C': 100} for n_compo= 45\n",
      "LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[110 106]\n",
      " [ 79 137]]\n",
      "[[14 24]\n",
      " [34 59]]\n",
      "precision of testing set: 0.7108433734939759\n",
      "recall of testing set 0.6344086021505376\n",
      "accuracy of testing set 0.5572519083969466\n",
      "f1 of testing set 0.6704545454545455\n",
      "Shape of X train: (432, 50)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5285964912280702\n",
      "With UMAP= 50 and l1, the best params are:\n",
      "{'C': 1000} for n_compo= 50\n",
      "LogisticRegression(C=1000, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[122  94]\n",
      " [ 85 131]]\n",
      "[[14 24]\n",
      " [37 56]]\n",
      "precision of testing set: 0.7\n",
      "recall of testing set 0.6021505376344086\n",
      "accuracy of testing set 0.5343511450381679\n",
      "f1 of testing set 0.6473988439306358\n",
      "Shape of X train: (432, 55)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5200108932461873\n",
      "With UMAP= 55 and l1, the best params are:\n",
      "{'C': 1000} for n_compo= 55\n",
      "LogisticRegression(C=1000, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[128  88]\n",
      " [ 99 117]]\n",
      "[[18 20]\n",
      " [42 51]]\n",
      "precision of testing set: 0.7183098591549296\n",
      "recall of testing set 0.5483870967741935\n",
      "accuracy of testing set 0.5267175572519084\n",
      "f1 of testing set 0.6219512195121951\n",
      "Shape of X train: (432, 60)\n",
      "Shape of y train: (432,)\n",
      "Mean score of precision of the best C: 0.5488290466130916\n",
      "With UMAP= 60 and l1, the best params are:\n",
      "{'C': 100} for n_compo= 60\n",
      "LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[157  59]\n",
      " [123  93]]\n",
      "[[23 15]\n",
      " [54 39]]\n",
      "precision of testing set: 0.7222222222222222\n",
      "recall of testing set 0.41935483870967744\n",
      "accuracy of testing set 0.4732824427480916\n",
      "f1 of testing set 0.5306122448979592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "# print(y_train_sampled)\n",
    "# print(X_train_scaled)\n",
    "print (\"Shape of final train and test sets:\", X_train_scaled.shape, X_test_scaled.shape)\n",
    "    \n",
    "C_options = [0.001, 0.01, 0.1, 1, 100, 1000]\n",
    "n_components = [20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# ### PCA\n",
    "\n",
    "# for n in n_components:\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X_train = pca.fit_transform(X_train_scaled)\n",
    "#     X_test = pca.transform(X_test_scaled)\n",
    "#     print(\"PCA reduced check\")\n",
    "    \n",
    "#     var = np.sum((pca.explained_variance_ratio_))\n",
    "#     print(\"Total variance covered:\", var)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "### UMAP\n",
    "\n",
    "for n in n_components:\n",
    "    umap = UMAP(n_components=n)\n",
    "    X_train = umap.fit_transform(X_train_scaled)\n",
    "    X_test = umap.transform(X_test_scaled)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# ICA\n",
    "# for n in n_components:\n",
    "#     ica = FastICA(n_components=n)\n",
    "#     X_train = ica.fit_transform(X_train_scaled)\n",
    "#     X_test = ica.transform(X_test_scaled)\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            'C': C_options,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    print(\"Shape of X train:\", X_train.shape)\n",
    "    print(\"Shape of y train:\", y_train_sampled.shape)\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000, penalty='l1', class_weight='balanced', tol=0.1, solver='saga'), \n",
    "                        param_grid=param_grid, \n",
    "                        scoring=\"precision\", \n",
    "                        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), \n",
    "                        n_jobs=-1)\n",
    "    grid.fit(X_train, y_train_sampled)\n",
    "    # evaluation metric is f1 scoring\n",
    "    print(\"Mean score of precision of the best C:\", grid.best_score_)\n",
    "    print(\"With UMAP=\",n,\"and l1, the best params are:\")\n",
    "    print(grid.best_params_, \"for n_compo=\", n)\n",
    "    \n",
    "    clf = grid.best_estimator_\n",
    "    print(clf)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    cm = confusion_matrix(y_train_sampled, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    pre = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    print(cm_test)\n",
    "    print(\"precision of testing set:\", pre)\n",
    "    print(\"recall of testing set\", recall)\n",
    "    print(\"accuracy of testing set\", acc)\n",
    "    print(\"f1 of testing set\", f1)\n",
    "    \n",
    "\n",
    "## L1 Regularisation\n",
    "# alpha=[0.001,0.01, 0.1]\n",
    "# for a in alpha:\n",
    "#     sel_ = SelectFromModel(Lasso(alpha=a))\n",
    "#     sel_.fit(X_train_scaled, y_train_sampled)\n",
    "#     X_train_selected = sel_.transform(X_train_scaled)\n",
    "#     X_test_selected = sel_.transform(X_test_scaled)\n",
    "\n",
    "#     param_grid = [\n",
    "#         {\n",
    "#             'C': C_options\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "#     ####Use L1 LR for clf\n",
    "#     grid = GridSearchCV(LogisticRegression(max_iter=500, penalty='l2', solver='saga'), param_grid=param_grid, scoring=\"accuracy\", cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), n_jobs=3)\n",
    "#     grid.fit(X_train_selected, y_train)\n",
    "#     mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "#     print(\"FS using Regularisation with alpha=\", a, \"and l2:\")\n",
    "#     print(grid.cv_results_['params'])\n",
    "#     print(mean_scores)\n",
    "#     print(grid.best_params_)\n",
    "\n",
    "\n",
    "# ###TEST for FS using different\n",
    "# ####Lasso:\n",
    "# sel_lasso = SelectFromModel(Lasso(alpha=0.01, tol=0.01))\n",
    "# sel_lasso.fit(X_train_scaled, y_train)\n",
    "# X_train_selected = sel_lasso.transform(X_train_scaled)\n",
    "# X_test_selected = sel_lasso.transform(X_test_scaled)\n",
    "# clf_lasso = LogisticRegression(max_iter=1000, penalty='l2', solver='saga', C=100).fit(X_train_selected, y_train)\n",
    "# y_pred_lasso = clf_lasso.predict(X_test_selected)\n",
    "\n",
    "# ####LogReg:\n",
    "# sel_logreg = SelectFromModel(LogisticRegression(C=1000, penalty='l1', solver='saga', tol=0.01))\n",
    "# sel_logreg.fit(X_train_scaled, y_train)\n",
    "# X_train_selected = sel_logreg.transform(X_train_scaled)\n",
    "# X_test_selected = sel_logreg.transform(X_test_scaled)\n",
    "# clf_logreg = LogisticRegression(max_iter=1000, penalty='l2', solver='saga', C=0.1).fit(X_train_selected, y_train)\n",
    "# y_pred_logreg = clf_logreg.predict(X_test_selected)\n",
    "\n",
    "# print(\"Accuracy of LassoFS+LogReg:\", accuracy_score(y_test, y_pred_lasso))\n",
    "# print(\"Accuracy of LogRegFS+LogReg:\", accuracy_score(y_test, y_pred_logreg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

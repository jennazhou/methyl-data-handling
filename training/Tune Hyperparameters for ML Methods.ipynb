{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ppmi = pd.read_csv('./trans_processed_PPMI_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi.rename(columns={'Unnamed: 0':'Sentrix_position'}, inplace=True)\n",
    "ppmi.set_index('Sentrix_position', inplace=True)\n",
    "ppmi = ppmi.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classifier on original unreduced data without anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "label = encoder.fit_transform(ppmi['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 747668)\n",
      "(436,)\n"
     ]
    }
   ],
   "source": [
    "tr = ppmi.drop(['Category'], axis=1)\n",
    "X = tr.values\n",
    "y = label\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 348 TEST: 88\n",
      "(348, 747668) (348,) (88, 747668) (88,)\n"
     ]
    }
   ],
   "source": [
    "#Stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "split.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "####所有的test都只能apply transform，不能用fit_transform!!!\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters for Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Train model using X_train for regulariser and C value strength\n",
    "###L1 first\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': C_options,\n",
    "    }\n",
    "]\n",
    "\n",
    "lr =  LogisticRegression(max_iter=500, penalty='l1', C=0.01, solver='saga')\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, scoring=\"accuracy\", cv=6, n_jobs=6)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = lr.predict\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Train model using X_train for regulariser and C value strength\n",
    "###L2 now\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': C_options,\n",
    "    }\n",
    "]\n",
    "\n",
    "lr =  LogisticRegression(max_iter=500, penalty='l2',solver='saga')\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, scoring=\"accuracy\", n_jobs=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With elasticnet\n",
    "l1_ratio = [0.2, 0.5, 0.8]\n",
    "param_grid = [\n",
    "    {\n",
    "        'l1_ratio': l1_ratio,\n",
    "    }\n",
    "]\n",
    "\n",
    "lr =  LogisticRegression(max_iter=500, penalty='elasticnet',solver='saga')\n",
    "\n",
    "grid = GridSearchCV(lr, param_grid=param_grid, scoring=\"accuracy\", n_jobs=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- cross_val_score: used as an analysis tool to evaluate the results obtained by training strategy used before (i.e. the model may be applied entirely)\n",
    "- cross_val_predict: apply cross-validation to training and get predictions and use it for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SGDClassifier with rbf kernel mapping\n",
    "\n",
    "###Approx SVC, hence set penalty to be l2\n",
    "\n",
    "# C_options = [0.01, 1, 100]\n",
    "# kernels=['rbf', 'poly', 'linear', 'sigmoid']\n",
    "# feature_map = Nystroem(gamma=1, random_state=1,n_components=300)\n",
    "# svm = SGDClassifier(penalty='l2', loss='hinge', tol=0.1)\n",
    "# svm_kernel_approx = Pipeline([\n",
    "#     (\"feature_map\", feature_map),\n",
    "#     (\"svm\", svm)\n",
    "# ])\n",
    "\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'feature_map__kernel': kernels,\n",
    "#         'svm__alpha': C_options,\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# svm_kernel_approx.fit(X_train, y_train)\n",
    "# y_pred_svm_approx = svm_kernel_approx.predict(X_test) \n",
    "\n",
    "### Conclusion: SVM without regularisation has \n",
    "### worse performance than Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SVC as svm\n",
    "###3 hypeparameters\n",
    "kernels = ['rbf', 'poly', 'linear', 'sigmoid']\n",
    "C_options=[0.01, 1, 1000]\n",
    "gamma=[1e-4, 0.01, 1, 1.5]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': C_options,\n",
    "        'kernel': kernels,\n",
    "        'gamma':gamma,\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"accuracy\", n_jobs=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of SVM: 0.7159090909090909\n",
      "Accuracy score of SGDClassifier with kernel approx: 0.7045454545454546\n"
     ]
    }
   ],
   "source": [
    "# print(\"Accuracy score of SVM:\", accuracy_score(y_test, y_pred_svm))\n",
    "# print(\"Accuracy score of SGDClassifier with kernel approx:\", accuracy_score(y_test, y_pred_svm_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "# DMatrix: a data structure that makes everything more efficient\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test, y_test)\n",
    "param = {'max_depth' : 3, 'eta' : 0.1, 'objective' : 'binary:logistic', 'seed' : 42, 'gpu_id': 0, 'tree_method': 'gpu_hist'}\n",
    "num_round = 50\n",
    "bst = xgb.train(param, dtrain, num_round, [(dtest, 'test'), (dtrain, 'train')])\n",
    "# y_pred = bst.predict(dtest)\n",
    "bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tune parameters for Dimensionality Reduction techniques + classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _1.1 PCA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######PCA on PPMI#########\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = [50, 100, 150, 200, 250]\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 PCA+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65217391 0.66393375 0.53138716 0.49714286 0.52004141]\n",
      "{'pca__n_components': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for PCA+Logistic Regression\n",
    "###L1\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l1'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'pca__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune n_components for PCA+Logistic Regression\n",
    "###L2\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l2'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'pca__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 PCA+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61797101 0.61490683 0.62062112 0.63813665 0.63233954]\n",
      "Best estimator:  {'pca__n_components': 200}\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for PCA+SVM\n",
    "\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "kernels = ['rbf', 'poly', 'linear', 'sigmoid']\n",
    "C_options=[0.01, 1, 1000]\n",
    "gamma=[1e-4, 0.01, 1, 1.5]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'pca__n_components': n_components,\n",
    "        'clf__C': C_options,\n",
    "        'clf__kernel': kernels,\n",
    "        'clf__gamma':gamma,\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\",n_jobs=3)\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------\n",
    "Conclusion so far:  \n",
    "Applying PCA technique reduces the accuracy of model when only running on PPMI dataset  \n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _1.2 UMAP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning UMAP hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 UMAP+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68968944 0.68968944 0.68964803 0.68389234 0.6810766 ]\n",
      "{'umap__n_components': 50}\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for UMAP+Logistic Regression\n",
    "###L1\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('umap', UMAP()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l1'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'umap__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune n_components for UMAP+Logistic Regression\n",
    "###L2\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('umap', UMAP()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l2'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'umap__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 UMAP+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68674948 0.69540373 0.68679089 0.68968944 0.68964803]\n",
      "Best estimator:  {'umap__n_components': 100}\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for UMAP+SVM\n",
    "\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "kernels = ['rbf', 'poly', 'linear', 'sigmoid']\n",
    "C_options=[0.01, 1, 1000]\n",
    "gamma=[1e-4, 0.01, 1, 1.5]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('umap', UMAP()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'umap__n_components': n_components,\n",
    "        'clf__C': C_options,\n",
    "        'clf__kernel': kernels,\n",
    "        'clf__gamma':gamma,\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\",n_jobs=3)\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ICA and Tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67817805 0.62927536 0.64057971 0.61494824 0.65229814]\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for ICA+Logistic Regression\n",
    "###L1\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('ica', FastICA()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l1'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'ica__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune n_components for ICA+Logistic Regression\n",
    "###L2\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "C_options = [0.01, 0.1, 1, 1.5, 10, 100]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('ica', FastICA()),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l2'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'ica__n_components': n_components\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62662526 0.62658385 0.62650104 0.58339545 0.63817805]\n",
      "Best estimator:  {'ica__n_components': 250}\n"
     ]
    }
   ],
   "source": [
    "### Tune n_components for ICA+SVM\n",
    "\n",
    "n_components = [50, 100, 150, 200, 250]\n",
    "kernels = ['rbf', 'poly', 'linear', 'sigmoid']\n",
    "C_options=[0.01, 1, 1000]\n",
    "gamma=[1e-4, 0.01, 1, 1.5]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('umap', UMAP()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'umap__n_components': n_components,\n",
    "        'clf__C': C_options,\n",
    "        'clf__kernel': kernels,\n",
    "        'clf__gamma':gamma,\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\",n_jobs=3)\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print('Best estimator: ', grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularisation to FS for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Use regularisation as Feature Selection technique to \n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####this returns the values that are positive after regularisation \n",
    "#Try different C value\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_.fit(X_train, y_train)\n",
    "#### sel_.get_support() returns a boolean matrix where True indicates the entries bigger than 0 and False otherwise\n",
    "# selected_feat = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Transform the original data to only the selected features based on regulariser\n",
    "X_train_selected = sel_.transform(X_train)\n",
    "X_test_selected = sel_.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune C value for regulariser for FS, then LR\n",
    "###L1\n",
    "C_options=[0.01, 1, 1000]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', SelectFromModel(LogisticRegression(C=1, penalty='l1'))),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l1'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'sel__estimator__C': C_options #??\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune C value for regulariser for FS, then LR\n",
    "###L2\n",
    "C_options=[0.01, 1, 1000]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', SelectFromModel(LogisticRegression(C=1, penalty='l1'))),\n",
    "    ('clf', LogisticRegression(max_iter=500, penalty='l2'))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'sel__estimator__C': C_options, #??\n",
    "        'clf__C': C_options\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune C value for regulariser for FS, then SVM\n",
    "\n",
    "C_options=[0.01, 1, 1000]\n",
    "kernels = ['rbf', 'poly']\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', SelectFromModel(LogisticRegression(C=1, penalty='l1'))),\n",
    "    ('clf', SVC(max_iter=500))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'sel__estimator__C': C_options, #??\n",
    "        'clf__C': C_options,\n",
    "        'clf__kernel': kernels\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# evaluation metric is accuray \n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(mean_scores)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. VAE DR + CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:CPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_CPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_GPU:1',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_GPU:2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental_list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 747668)\n",
      "(436,)\n",
      "TRAIN: 348 TEST: 88\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 747668)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 400)          299067600   encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 100)          40100       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 100)          40100       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 100)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 299,147,800\n",
      "Trainable params: 299,147,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               40400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 747668)            299814868 \n",
      "=================================================================\n",
      "Total params: 299,855,268\n",
      "Trainable params: 299,855,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-w WEIGHTS] [-m]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /auto/homes/rz296/.local/share/jupyter/runtime/kernel-83d28ab7-a33d-4dbb-8605-396cf6fb50f4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "###Variational Autoencoder to get the latent layer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import keras\n",
    "\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "ppmi = pd.read_csv('./trans_processed_PPMI_data.csv')\n",
    "ppmi.rename(columns={'Unnamed: 0':'Sentrix_position'}, inplace=True)\n",
    "ppmi.set_index('Sentrix_position', inplace=True)\n",
    "ppmi = ppmi.transpose()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "label = encoder.fit_transform(ppmi['Category'])\n",
    "\n",
    "tr = ppmi.drop(['Category'], axis=1)\n",
    "X = tr.values\n",
    "y = label\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "split.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom variational layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    This function is borrowed from:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise variables and Set hyperparameters\n",
    "original_dim = X.shape[1] #747668\n",
    "latent_dim = 100\n",
    "\n",
    "batch_size = 50 # controls the number of training samples to \n",
    "                # work through before the model's internal parameters are updated\n",
    "epochs = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "\n",
    "############ENCODER##################\n",
    "inputs = Input(shape=(original_dim, ), name='encoder_input')\n",
    "#mean and log_var are the vectors of size `latent_dim`\n",
    "z_mean_linear = Dense(latent_dim, kernel_initializer='glorot_uniform', name='z_mean')(inputs)\n",
    "z_mean_batchnorm = BatchNormalization()(z_mean_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_batchnorm)\n",
    "\n",
    "z_log_var_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(inputs)\n",
    "z_log_var_batchnorm = BatchNormalization()(z_log_var_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_batchnorm)\n",
    "\n",
    "# return the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a `latent_dim` output\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "\n",
    "#### actually not very sure how and why use sampling??#####\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "# Model to compress input\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "\n",
    "############DECODER##################\n",
    "decoder_to_reconstruct = Dense(original_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "reconstructed_output = decoder_to_reconstruct(z)\n",
    "\n",
    "########### instantiate VAE model##########\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([inputs, reconstructed_output])\n",
    "vae = Model(inputs, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "    \n",
    "#Train the model\n",
    "vae.fit(X_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, X_test))\n",
    "\n",
    "score = vae.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import talos\n",
    "from talos.utils.gpu_utils import multi_gpu\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hyperparameters for MLP\n",
    "p = {'lr': (0.8, 1.0, 3),\n",
    "     'first_neuron':[256, 512],#,32, 64, 128\n",
    "     'hidden_layer_neuron':[32], #64, 128, 256\n",
    "     'batch_size': [10], #, 20, 30\n",
    "     'epochs': [10], #, 20, 30\n",
    "     'dropout': [0],#this means try every .1 value between 0 and .5\n",
    "     'kernel_initializer': ['uniform'], #,'normal'\n",
    "     'optimizer': ['Adam'], #'Nadam'\n",
    "     'losses': ['binary_crossentropy'],\n",
    "     'activation':['relu'], #, 'elu', 'tanh'\n",
    "     'last_activation': ['sigmoid'] #, 'softmax'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\";  \n",
    "def mlp_model(x_train, y_train, x_val, y_val, params):\n",
    "    # Build the model.\n",
    "    # Anyhow give parameters first\n",
    "    mlp = Sequential([\n",
    "      Dense(params['first_neuron'], activation=params['activation'], input_shape=(747668,), kernel_initializer=params['kernel_initializer']),\n",
    "      Dropout(params['dropout'], seed=42),\n",
    "      Dense(params['hidden_layer_neuron'], activation=params['activation']),\n",
    "      Dropout(params['dropout'], seed=42),\n",
    "    #   Dense(8, activation='relu'),\n",
    "    #   Dropout(Dropout(0.50, seed=42)),\n",
    "      Dense(2, activation=params['last_activation']),\n",
    "    ])\n",
    "\n",
    "    # split a single job to multiple GPUs\n",
    "#     mlp = multi_gpu(mlp)\n",
    "\n",
    "    # Compile the model\n",
    "    mlp.compile(\n",
    "      optimizer=params['optimizer'],\n",
    "      loss=params['losses'],\n",
    "      metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    # Train the data\n",
    "    history = mlp.fit(\n",
    "      x_train, # training data\n",
    "      to_categorical(y_train), # training targets\n",
    "      epochs=params['epochs'],\n",
    "      batch_size=params['batch_size'],\n",
    "    )\n",
    "\n",
    "    return history, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 99.2364 - accuracy: 0.5720\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 50.4819 - accuracy: 0.7737\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 26.3427 - accuracy: 0.8148\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 6.0931 - accuracy: 0.9300\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 6.6742 - accuracy: 0.9465\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 5.2511 - accuracy: 0.9486\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 9.7458 - accuracy: 0.9342\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.9741 - accuracy: 0.9527\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 6.6655 - accuracy: 0.9465\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.7224 - accuracy: 0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 1/6 [03:45<18:46, 225.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 162.5748 - accuracy: 0.5597\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 83.9551 - accuracy: 0.7922\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 40.1386 - accuracy: 0.8683\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 23.6784 - accuracy: 0.9321\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 31.4243 - accuracy: 0.9403\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 4.7200 - accuracy: 0.9588\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 11.4621 - accuracy: 0.9506\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 2.9521 - accuracy: 0.9712\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 6.0054 - accuracy: 0.9630\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 2.5066 - accuracy: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [07:29<15:00, 225.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 116.8592 - accuracy: 0.5597\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 55.3527 - accuracy: 0.7634\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 18.3971 - accuracy: 0.8313\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 4.8842 - accuracy: 0.8909\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 1.0156 - accuracy: 0.9136\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 22s 91ms/step - loss: 0.4847 - accuracy: 0.9280\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.4403 - accuracy: 0.9342\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.6235 - accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 22s 90ms/step - loss: 0.6140 - accuracy: 0.9383\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 22s 89ms/step - loss: 0.3334 - accuracy: 0.9362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 3/6 [11:14<11:15, 225.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 169.2561 - accuracy: 0.5802\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 78.4825 - accuracy: 0.7387\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 23.3497 - accuracy: 0.8230\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 11.9267 - accuracy: 0.8889\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 5.2280 - accuracy: 0.9053\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 43s 179ms/step - loss: 0.4986 - accuracy: 0.9095\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 0.2300 - accuracy: 0.8971\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.1773 - accuracy: 0.8642\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.1363 - accuracy: 0.9136\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.1212 - accuracy: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [18:41<09:43, 291.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 171.5801 - accuracy: 0.5802\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 45.4014 - accuracy: 0.7243\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 9.8214 - accuracy: 0.7942\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 8.6678 - accuracy: 0.8004\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 2.4885 - accuracy: 0.8086\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 2.0026 - accuracy: 0.7963\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 1.3848 - accuracy: 0.8066\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 44s 183ms/step - loss: 0.3892 - accuracy: 0.8045\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 0.3906 - accuracy: 0.8025\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.3893 - accuracy: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [26:12<05:39, 339.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 43s 179ms/step - loss: 169.7645 - accuracy: 0.6111\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 36.7431 - accuracy: 0.7922\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 44s 179ms/step - loss: 11.6468 - accuracy: 0.8683\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 44s 183ms/step - loss: 2.3361 - accuracy: 0.8807\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 1.4793 - accuracy: 0.8992\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 44s 180ms/step - loss: 0.2897 - accuracy: 0.9198\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 44s 181ms/step - loss: 0.1192 - accuracy: 0.9239\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 45s 183ms/step - loss: 0.1588 - accuracy: 0.9239\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 44s 182ms/step - loss: 0.3978 - accuracy: 0.9177\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 45s 184ms/step - loss: 1.7546 - accuracy: 0.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6/6 [34:00<00:00, 340.03s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#Tune hyperparameters using talos\n",
    "## To start fast, limit the permutation to 1/100 of the original permutation\n",
    "scan_object = talos.Scan(x=X_train,\n",
    "                         y=y_train, \n",
    "                         params=p,\n",
    "                         model=mlp_model,\n",
    "                         experiment_name='mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.722449</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.333380</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.934156</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.389267</td>\n",
       "      <td>0.804527</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1.754559</td>\n",
       "      <td>0.911523</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs      loss  accuracy activation  batch_size  dropout  epochs  \\\n",
       "0            10  2.722449  0.958848       relu          10        0      10   \n",
       "1            10  2.506606  0.971193       relu          10        0      10   \n",
       "2            10  0.333380  0.936214       relu          10        0      10   \n",
       "3            10  0.121178  0.934156       relu          10        0      10   \n",
       "4            10  0.389267  0.804527       relu          10        0      10   \n",
       "5            10  1.754559  0.911523       relu          10        0      10   \n",
       "\n",
       "   first_neuron  hidden_layer_neuron kernel_initializer last_activation  \\\n",
       "0           256                   32            uniform         sigmoid   \n",
       "1           256                   32            uniform         sigmoid   \n",
       "2           256                   32            uniform         sigmoid   \n",
       "3           512                   32            uniform         sigmoid   \n",
       "4           512                   32            uniform         sigmoid   \n",
       "5           512                   32            uniform         sigmoid   \n",
       "\n",
       "                losses        lr optimizer  \n",
       "0  binary_crossentropy  0.800000      Adam  \n",
       "1  binary_crossentropy  0.866667      Adam  \n",
       "2  binary_crossentropy  0.933333      Adam  \n",
       "3  binary_crossentropy  0.800000      Adam  \n",
       "4  binary_crossentropy  0.866667      Adam  \n",
       "5  binary_crossentropy  0.933333      Adam  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.612864</td>\n",
       "      <td>2.292603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.627731</td>\n",
       "      <td>2.292070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.081373</td>\n",
       "      <td>2.292943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.102660</td>\n",
       "      <td>2.294472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954518</td>\n",
       "      <td>2.298395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.808678</td>\n",
       "      <td>2.296348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  1.612864  2.292603\n",
       "1  1.627731  2.292070\n",
       "2  1.081373  2.292943\n",
       "3  1.102660  2.294472\n",
       "4  0.954518  2.298395\n",
       "5  0.808678  2.296348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "experiment_name                     mlp\n",
       "random_method          uniform_mersenne\n",
       "reduction_method                   None\n",
       "reduction_interval                   50\n",
       "reduction_window                     20\n",
       "reduction_threshold                 0.2\n",
       "reduction_metric                val_acc\n",
       "complete_time            02/03/20/22:25\n",
       "x_shape                   (348, 747668)\n",
       "y_shape                          (348,)\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing the results data frame\n",
    "display(scan_object.data)\n",
    "\n",
    "# accessing epoch entropy values for each round\n",
    "display(scan_object.learning_entropy)\n",
    "\n",
    "# access the summary details\n",
    "scan_object.details\n",
    "\n",
    "####Documentation for training: as the number of neuron increases, accuracy does not necessarily increase, however the time taken for training increases significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.722449</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.333380</td>\n",
       "      <td>0.936214</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.934156</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1.754559</td>\n",
       "      <td>0.911523</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.389267</td>\n",
       "      <td>0.804527</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs      loss  accuracy activation  batch_size  dropout  epochs  \\\n",
       "1            10  2.506606  0.971193       relu          10        0      10   \n",
       "0            10  2.722449  0.958848       relu          10        0      10   \n",
       "2            10  0.333380  0.936214       relu          10        0      10   \n",
       "3            10  0.121178  0.934156       relu          10        0      10   \n",
       "5            10  1.754559  0.911523       relu          10        0      10   \n",
       "4            10  0.389267  0.804527       relu          10        0      10   \n",
       "\n",
       "   first_neuron  hidden_layer_neuron kernel_initializer last_activation  \\\n",
       "1           256                   32            uniform         sigmoid   \n",
       "0           256                   32            uniform         sigmoid   \n",
       "2           256                   32            uniform         sigmoid   \n",
       "3           512                   32            uniform         sigmoid   \n",
       "5           512                   32            uniform         sigmoid   \n",
       "4           512                   32            uniform         sigmoid   \n",
       "\n",
       "                losses        lr optimizer  \n",
       "1  binary_crossentropy  0.866667      Adam  \n",
       "0  binary_crossentropy  0.800000      Adam  \n",
       "2  binary_crossentropy  0.933333      Adam  \n",
       "3  binary_crossentropy  0.800000      Adam  \n",
       "5  binary_crossentropy  0.933333      Adam  \n",
       "4  binary_crossentropy  0.866667      Adam  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_so = scan_object.data.sort_values(by=[\"accuracy\"], ascending=False)\n",
    "sorted_so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2.908766e-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.506606e+00</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs          loss  accuracy activation  batch_size  dropout  \\\n",
       "3            10  2.908766e-14  1.000000       relu          10        0   \n",
       "1            10  2.506606e+00  0.971193       relu          10        0   \n",
       "\n",
       "   epochs  first_neuron  hidden_layer_neuron kernel_initializer  \\\n",
       "3      10            64                   32            uniform   \n",
       "1      10           256                   32            uniform   \n",
       "\n",
       "  last_activation               losses        lr optimizer  \n",
       "3         sigmoid  binary_crossentropy  0.800000      Adam  \n",
       "1         sigmoid  binary_crossentropy  0.866667      Adam  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layer_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2.908766e-14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>uniform</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs          loss  accuracy activation  batch_size  dropout  \\\n",
       "3            10  2.908766e-14       1.0       relu          10        0   \n",
       "\n",
       "   epochs  first_neuron  hidden_layer_neuron kernel_initializer  \\\n",
       "3      10            64                   32            uniform   \n",
       "\n",
       "  last_activation               losses   lr optimizer  \n",
       "3         sigmoid  binary_crossentropy  0.8      Adam  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(sorted_so.iloc[:1])\n",
    "# final_params_info = df1\n",
    "df = pd.concat([df1, final_params_info]).sort_values(by=['accuracy'], ascending=False)\n",
    "display(df)\n",
    "\n",
    "final_params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear GPU memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100.73245377974077, 0.6477272510528564]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on Test set\n",
    "# evaluate() returns an array containing the test loss followed by any metrics we specified. \n",
    "mlp.evaluate(\n",
    "  X_test,\n",
    "  to_categorical(y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

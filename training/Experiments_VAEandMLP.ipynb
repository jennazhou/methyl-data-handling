{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### VAE DR + CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "# tf.config.experimental_list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###Variational Autoencoder to get the latent layer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import keras\n",
    "\n",
    "import pydot\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT1\n",
      "CHECKPOINT2\n",
      "CHECKPOINT3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rz296/miniconda3/envs/partII/lib/python3.6/multiprocessing/popen_fork.py:73: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  code = process_obj._bootstrap()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436, 747668)\n",
      "(436,)\n",
      "StratifiedSampling check\n",
      "Oversampling check\n",
      "Scaling check\n",
      "Returning check\n"
     ]
    }
   ],
   "source": [
    "# def set_Data(data):\n",
    "#     ppmi = pd.read_csv('../../datasets/preprocessed/trans_processed_PPMI_data.csv')\n",
    "#     ppmi.rename(columns={'Unnamed: 0':'Sentrix_position'}, inplace=True)\n",
    "#     ppmi.set_index('Sentrix_position', inplace=True)\n",
    "#     ppmi = ppmi.transpose()\n",
    "\n",
    "#     encoder = LabelEncoder()\n",
    "#     label = encoder.fit_transform(ppmi['Category'])\n",
    "\n",
    "#     tr = ppmi.drop(['Category'], axis=1)\n",
    "#     X = tr.values\n",
    "#     y = label\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "\n",
    "#     print(\"StratifiedSampling check\")\n",
    "#     split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     split.get_n_splits(X, y)\n",
    "\n",
    "#     for train_index, test_index in split.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, data['y_test'] = y[train_index], y[test_index]\n",
    "\n",
    "#     print(\"Oversampling check\")\n",
    "#     oversampler = SMOTE(random_state=42)\n",
    "#     X_train_sampled, data['y_train_sampled'] = oversampler.fit_resample(X_train, y_train)\n",
    "#     print(\"Scaling check\")\n",
    "#     scaler = StandardScaler()\n",
    "# #     scaler = MinMaxScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train_sampled)\n",
    "#     data['X_train_scaled_1'] = X_train_scaled[:247].reshape((1, -1))\n",
    "#     data['X_train_scaled_2'] = X_train_scaled[247:].reshape((1, -1))\n",
    "#     data['X_test_scaled'] = scaler.transform(X_test)\n",
    "    \n",
    "#     print(\"Returning check\")\n",
    "\n",
    "# manager = Manager()\n",
    "# data = manager.dict()\n",
    "\n",
    "# print(\"CHECKPOINT1\")\n",
    "# #     p = Process(target=set_Data, args=(X_train_scaled, X_test_scaled, y_train_sampled, y_test,))\n",
    "# p = Process(target=set_Data, args=(data,))\n",
    "# print(\"CHECKPOINT2\")\n",
    "# p.start()\n",
    "# print(\"CHECKPOINT3\")\n",
    "# p.join()\n",
    "\n",
    "# y_train = data['y_train_sampled']\n",
    "# y_test = data['y_test']\n",
    "# X_train = np.append(data['X_train_scaled_1'], data['X_train_scaled_2']).reshape(494, 747668)\n",
    "# X_test = data['X_test_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from .npy files\n",
    "y_train_sampled = np.load('../../datasets/preprocessed/npy_files/y_train_sampled.npy')\n",
    "y_test = np.load('../../datasets/preprocessed/npy_files/y_test.npy')\n",
    "X_train_scaled = np.load('../../datasets/preprocessed/npy_files/X_train_scaled.npy')\n",
    "X_test_scaled = np.load('../../datasets/preprocessed/npy_files/X_test_scaled.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_ppg = np.load('../../datasets/preprocessed/npy_files/X_scaled_ppg.npy')\n",
    "y_ppg = np.load('../../datasets/preprocessed/npy_files/y_ppg.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "#     epsilon = K.random_normal(shape=(batch, dim))\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom variational layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    This function is borrowed from:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise variables and Set hyperparameters\n",
    "original_dim = X_train.shape[1] #747668\n",
    "latent_dim = 100\n",
    "\n",
    "batch_size = 50 # controls the number of training samples to \n",
    "                # work through before the model's internal parameters are updated\n",
    "epochs = 50\n",
    "beta = K.variable(0)\n",
    "kappa = 1\n",
    "\n",
    "learning_rate = 0.05\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-97183140b9d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m########### instantiate VAE model##########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mvae_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomVariationalLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/partII/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/partII/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f2650ccd7e9>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_decoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# We won't actually use the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8f2650ccd7e9>\u001b[0m in \u001b[0;36mvae_loss\u001b[0;34m(self, x_input, x_decoded)\u001b[0m\n\u001b[1;32m     15\u001b[0m         kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n\u001b[1;32m     16\u001b[0m                                 K.exp(z_log_var_encoded), axis=-1)\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/partII/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2925\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/partII/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     raise NotImplementedError(\n\u001b[0;32m--> 579\u001b[0;31m         \"numpy() is only available when eager execution is enabled.\")\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Prefer Dataset.range instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# VAE model = encoder + decoder\n",
    "\n",
    "############ENCODER##################\n",
    "inputs = Input(shape=(original_dim, ), name='encoder_input')\n",
    "#mean and log_var are the vectors of size `latent_dim`\n",
    "z_mean_linear = Dense(latent_dim, kernel_initializer='glorot_uniform', name='z_mean')(inputs)\n",
    "z_mean_batchnorm = BatchNormalization()(z_mean_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_batchnorm)\n",
    "\n",
    "z_log_var_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(inputs)\n",
    "z_log_var_batchnorm = BatchNormalization()(z_log_var_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_batchnorm)\n",
    "\n",
    "# return the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a `latent_dim` output\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "\n",
    "#### actually not very sure how and why use sampling??#####\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "# Model to compress input\n",
    "encoder = Model(inputs, z_mean_encoded)\n",
    "\n",
    "\n",
    "############DECODER##################\n",
    "decoder_to_reconstruct = Dense(original_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "reconstructed_output = decoder_to_reconstruct(z)\n",
    "\n",
    "########### instantiate VAE model##########\n",
    "adam = optimizers.Adam(lr=0.3)\n",
    "vae_layer = CustomVariationalLayer()([inputs, reconstructed_output])\n",
    "vae = Model(inputs, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "\n",
    "    \n",
    "#Train the model\n",
    "vae.fit(X_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, X_test))\n",
    "\n",
    "score = vae.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters for MLP\n",
    "params = {'learning_rate': [0.3], #0.1\n",
    "     'l1_n':[128],#128, 512\n",
    "     'l2_n':[20], #64, 128\n",
    "     'dropout': [0.5],#0.7, 0.9\n",
    "     'act':['relu'],\n",
    "     'last_act': ['softmax'] #, 'softmax'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score\n",
    "# def prec_score(y_true, y_pred):\n",
    "#     return precision_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(x_train, y_train, x_val, y_val, params):\n",
    "    # Build the model.\n",
    "    # Anyhow give parameters first\n",
    "    mlp = Sequential([\n",
    "      Dense(512, activation=params['act'], input_shape=(747668,), \n",
    "            kernel_initializer='normal', kernel_regularizer=regularizers.l2(1000), bias_regularizer=regularizers.l2(1000), activity_regularizer=regularizers.l1(1000)),\n",
    "      Dropout(0.5, seed=42),\n",
    "      Dense(128, activation=params['act'], kernel_regularizer=regularizers.l2(1000), bias_regularizer=regularizers.l2(1000), activity_regularizer=regularizers.l1(1000)),\n",
    "      Dropout(0.3, seed=42),\n",
    "      Dense(16, activation=params['act'], kernel_regularizer=regularizers.l2(100), bias_regularizer=regularizers.l2(100), activity_regularizer=regularizers.l1(100)),\n",
    "      Dropout(0.1, seed=42),\n",
    "    #   Dense(8, activation='relu'),\n",
    "    #   Dropout(Dropout(0.50, seed=42)),\n",
    "      Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    mlp.compile(\n",
    "      optimizer=optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "      loss='mean_squared_error',\n",
    "      metrics=[tf.keras.metrics.Precision(), 'accuracy'],\n",
    "    )\n",
    "\n",
    "    # Train the data\n",
    "    history = mlp.fit(\n",
    "        X_train_scaled, # training data\n",
    "        y_train_sampled, # training targets\n",
    "        epochs=20, # need to manually tune\n",
    "        batch_size=10, # need to manually tune\n",
    "        verbose=1,\n",
    "        validation_data=(X_test_scaled, y_test)\n",
    "    )\n",
    "\n",
    "    return history, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494 samples, validate on 88 samples\n",
      "Epoch 1/20\n",
      "494/494 [==============================] - 158s 321ms/step - loss: 104313212077.6032 - precision: 0.4907 - accuracy: 0.4858 - val_loss: 88388666647.2727 - val_precision: 0.4878 - val_accuracy: 0.2955\n",
      "Epoch 2/20\n",
      "494/494 [==============================] - 148s 299ms/step - loss: 95663997209.9109 - precision: 0.4943 - accuracy: 0.5020 - val_loss: 110472547421.0909 - val_precision: 0.5178 - val_accuracy: 0.6932\n",
      "Epoch 3/20\n",
      "494/494 [==============================] - 149s 301ms/step - loss: 98490383322.6882 - precision: 0.5046 - accuracy: 0.4575 - val_loss: 86633068637.0909 - val_precision: 0.5189 - val_accuracy: 0.6932\n",
      "Epoch 4/20\n",
      "494/494 [==============================] - 149s 301ms/step - loss: 78454611180.3077 - precision: 0.5234 - accuracy: 0.5081 - val_loss: 56567073419.6364 - val_precision: 0.5206 - val_accuracy: 0.2955\n",
      "Epoch 5/20\n",
      "494/494 [==============================] - 150s 304ms/step - loss: 94483199610.2996 - precision: 0.5220 - accuracy: 0.5385 - val_loss: 85814077067.6364 - val_precision: 0.5245 - val_accuracy: 0.2955\n",
      "Epoch 6/20\n",
      "494/494 [==============================] - 150s 303ms/step - loss: 81852083461.1822 - precision: 0.5212 - accuracy: 0.4879 - val_loss: 62447008023.2727 - val_precision: 0.5183 - val_accuracy: 0.2955\n",
      "Epoch 7/20\n",
      "494/494 [==============================] - 150s 303ms/step - loss: 78131592022.0243 - precision: 0.5163 - accuracy: 0.5061 - val_loss: 59650717137.4545 - val_precision: 0.5229 - val_accuracy: 0.7045\n",
      "Epoch 8/20\n",
      "494/494 [==============================] - 149s 302ms/step - loss: 79486831280.1943 - precision: 0.5232 - accuracy: 0.4879 - val_loss: 63691924200.7273 - val_precision: 0.5216 - val_accuracy: 0.2955\n",
      "Epoch 9/20\n",
      "494/494 [==============================] - 151s 305ms/step - loss: 84870201244.5020 - precision: 0.5219 - accuracy: 0.5182 - val_loss: 63835267630.5455 - val_precision: 0.5215 - val_accuracy: 0.2955\n",
      "Epoch 10/20\n",
      "494/494 [==============================] - 157s 318ms/step - loss: 92837456046.1215 - precision: 0.5211 - accuracy: 0.4960 - val_loss: 84306173486.5455 - val_precision: 0.5228 - val_accuracy: 0.7045\n",
      "Epoch 11/20\n",
      "494/494 [==============================] - 157s 318ms/step - loss: 89494715570.2672 - precision: 0.5213 - accuracy: 0.4413 - val_loss: 126110708084.3636 - val_precision: 0.5179 - val_accuracy: 0.2955\n",
      "Epoch 12/20\n",
      "140/494 [=======>......................] - ETA: 1:49 - loss: 106038322614.8571 - precision: 0.5177 - accuracy: 0.4786"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "\n",
    "hist_dict={}\n",
    "for lr in params['learning_rate']:\n",
    "    for l1_n in params['l1_n']:\n",
    "        for l2_n in params['l2_n']:\n",
    "            for do in params['dropout']:\n",
    "                for act in params['act']:\n",
    "                    for last_act in params['last_act']:\n",
    "                        cur_p = {\n",
    "                             'learning_rate': lr,\n",
    "                             'l1_n':l1_n,\n",
    "                             'l2_n':l2_n,\n",
    "                             'dropout': do,\n",
    "                             'act':act,\n",
    "                             'last_act': last_act\n",
    "                        } \n",
    "                        # print('Cur model:', cur_p)\n",
    "#                         params_results_df = params_results_df.append(cur_p, ignore_index=True)\n",
    "                        history, mlp = mlp_model(X_train_scaled, y_train_sampled, X_test_scaled, y_test, cur_p)\n",
    "                        hist_dict[c] = history.history\n",
    "\n",
    "                        # Visualize training performance\n",
    "                        history_df = pd.DataFrame(hist_dict[c])\n",
    "                        # hist_plot_file = os.path.join('figures', 'onehidden_vae_training.pdf')\n",
    "                        ax = history_df.plot()\n",
    "                        ax.set_xlabel('Epochs')\n",
    "                        ax.set_ylabel('MLP Loss')\n",
    "                        fig = ax.get_figure()\n",
    "                        fig.show()\n",
    "                        # fig.savefig(hist_plot_file)\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ### on PPMI testing\n",
    "# y_pred_ppmi = mlp.predict(X_test_scaled)\n",
    "y_pred_ppmi = mlp.predict(X_test_scaled).flatten()\n",
    "print(\"Prediction of ppmi before rounding:\")\n",
    "print(y_pred_ppmi)\n",
    "y_pred_ppmi = y_pred_ppmi.round()\n",
    "cm_ppmi = confusion_matrix(y_test, y_pred_ppmi)\n",
    "ppmi_prec = precision_score(y_test, y_pred_ppmi)\n",
    "ppmi_acc = accuracy_score(y_test, y_pred_ppmi)\n",
    "print(cm_ppmi)\n",
    "\n",
    "### on PPG\n",
    "y_pred_ppg =(mlp.predict(X_scaled_ppg)).round()\n",
    "cm_ppg = confusion_matrix(y_ppg, y_pred_ppg)\n",
    "print(cm_ppg)\n",
    "ppg_prec = precision_score(y_ppg, y_pred_ppg)\n",
    "ppg_acc = accuracy_score(y_ppg, y_pred_ppg)\n",
    "# print(\"PPG precision:\", prec)\n",
    "# print(\"PPG accuracy:\", acc)\n",
    "\n",
    "### Add to dictionary\n",
    "params_results_df[\"ppmi_accuracy\"] = ppmi_acc\n",
    "params_results_df[\"ppmi_precision\"] = ppmi_prec\n",
    "params_results_df[\"ppg_accuracy\"] = ppg_acc\n",
    "params_results_df[\"ppg_precision\"] = ppg_prec\n",
    "\n",
    "print (hist_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
